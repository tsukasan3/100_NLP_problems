{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第5章: 係り受け解析\n",
    "\n",
    "夏目漱石の小説『吾輩は猫である』の文章（[neko.txt](https://nlp100.github.io/data/neko.txt)）をCaboChaを使って係り受け解析し，その結果をneko.txt.cabochaというファイルに保存せよ．このファイルを用いて，以下の問に対応するプログラムを実装せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 40. 係り受け解析結果の読み込み（形態素）\n",
    "\n",
    "形態素を表すクラスMorphを実装せよ．このクラスは表層形（surface），基本形（base），品詞（pos），品詞細分類1（pos1）をメンバ変数に持つこととする．さらに，CaboChaの解析結果（neko.txt.cabocha）を読み込み，各文をMorphオブジェクトのリストとして表現し，3文目の形態素列を表示せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "neko_cabocha_file = '/Users/seiji/言語処理100本ノック/5係り受け解析/neko.txt.cabocha'\n",
    "\n",
    "\n",
    "\n",
    "class Morph:\n",
    "    \"\"\"\n",
    "    1つの形態素を表すクラス\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, surface, base, pos, pos1):\n",
    "        \"\"\"\n",
    "        メンバ変数として表層形（surface）, 基本形（base）, 品詞（pos）,　品詞細分類１（pos1）を持つ\n",
    "        \"\"\"\n",
    "        self.surface = surface\n",
    "        self.base = base\n",
    "        self.pos = pos\n",
    "        self.pos1 = pos1\n",
    "    \n",
    "    \n",
    "def make_morph_list(filename):\n",
    "    \"\"\"\n",
    "    係り受け解析済みのファイルを読み込み、各文をMorphオブジェクトのリストとして表現する\n",
    "    \"\"\"\n",
    "    \n",
    "    sentences = []\n",
    "    sentence = []\n",
    "    with open(filename, encoding='utf-8') as input_file:\n",
    "        for line in input_file:\n",
    "            if line[0] == '*':\n",
    "                next\n",
    "            if '\\t' in line:\n",
    "                item = line.strip().split('\\t')\n",
    "                try:\n",
    "                    surf = item[0]\n",
    "                    items = item[1].split(',')\n",
    "                except IndexError:\n",
    "                    next\n",
    "                if not item == ['記号,空白,*,*,*,*,\\u3000,\\u3000,']: #　空白の箇所の処理、\\u3000は空白　\n",
    "                    sentence.append(Morph(surf, items[6], items[0], items[1]))\n",
    "                    \n",
    "            \n",
    "                    if '句点' in item[1]:\n",
    "                        sentences.append(sentence)\n",
    "                        sentence = []\n",
    "                    \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "morphed_sentences = make_morph_list(neko_cabocha_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "surface: どこ, base: どこ, pos: 名詞, pos1: 代名詞\n",
      "surface: で, base: で, pos: 助詞, pos1: 格助詞\n",
      "surface: 生れ, base: 生れる, pos: 動詞, pos1: 自立\n",
      "surface: た, base: た, pos: 助動詞, pos1: *\n",
      "surface: かとん, base: 火遁, pos: 名詞, pos1: 一般\n",
      "surface: と, base: と, pos: 助詞, pos1: 格助詞\n",
      "surface: 見当, base: 見当, pos: 名詞, pos1: サ変接続\n",
      "surface: が, base: が, pos: 助詞, pos1: 格助詞\n",
      "surface: つか, base: つく, pos: 動詞, pos1: 自立\n",
      "surface: ぬ, base: ぬ, pos: 助動詞, pos1: *\n",
      "surface: 。, base: 。, pos: 記号, pos1: 句点\n"
     ]
    }
   ],
   "source": [
    "for item in morphed_sentences[2]:\n",
    "    print ('surface: {}, base: {}, pos: {}, pos1: {}'.format(item.surface, item.base, item.pos, item.pos1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 41. 係り受け解析結果の読み込み（文節・係り受け）\n",
    "\n",
    "40に加えて，文節を表すクラスChunkを実装せよ．このクラスは形態素（Morphオブジェクト）のリスト（morphs），係り先文節インデックス番号（dst），係り元文節インデックス番号のリスト（srcs）をメンバ変数に持つこととする．さらに，入力テキストのCaboChaの解析結果を読み込み，１文をChunkオブジェクトのリストとして表現し，8文目の文節の文字列と係り先を表示せよ．第5章の残りの問題では，ここで作ったプログラムを活用せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "neko_cabocha_file = '/Users/seiji/言語処理100本ノック/5係り受け解析/neko.txt.cabocha'\n",
    "\n",
    "class Morph(object):\n",
    "    def __init__(self, line):\n",
    "        self.surface, rest = line.split('\\t')\n",
    "        attr = rest.split(',')\n",
    "        self.base = attr[6]\n",
    "        self.pos = attr[0]\n",
    "        self.pos1 = attr[1]\n",
    "        \n",
    "    def __str__(self):\n",
    "        return 'surface:{}/base:{}/pos:{}/pos1:{}\\n'.format(\n",
    "            self.surface, self.base, self.pos, self.pos1\n",
    "        )\n",
    "\n",
    "class Chunk(object):\n",
    "    def __init__(self, line):\n",
    "        self.morphs = []\n",
    "        self.dst = int(line.split(' ')[2].rstrip('D'))\n",
    "        self.srcs = []\n",
    "\n",
    "def read_cabocha_file(filename):\n",
    "    sentence = []\n",
    "    chunk = None\n",
    "    with open(filename, mode='rt', encoding='utf-8') as input_file:\n",
    "        for line in input_file:\n",
    "            line = line.rstrip('\\n')\n",
    "            if line == 'EOS':\n",
    "                #sentenceのsrcを設定していく\n",
    "                for index, chunk in enumerate(sentence):\n",
    "                    # 係り先が -1の時は係り先がないから飛ばす\n",
    "                    if chunk.dst != -1:\n",
    "                        sentence[chunk.dst].srcs.append(index)\n",
    "                                \n",
    "                yield sentence\n",
    "                sentence = []\n",
    "            elif re.match(r'^\\*[^\\t]+$', line):\n",
    "                #'*'が形態素になってる可能性もあるので、\n",
    "                #タブを含まない行の行頭が '*'　な場合だけ読み飛ばす\n",
    "                chunk = Chunk(line)\n",
    "                sentence.append(chunk)\n",
    "            else:\n",
    "                chunk.morphs.append(Morph(line))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*  0\n",
      "srcs: [], dst: 5\n",
      "morphs: surface:吾輩/base:吾輩/pos:名詞/pos1:代名詞\n",
      "surface:は/base:は/pos:助詞/pos1:係助詞\n",
      "\n",
      "*  1\n",
      "srcs: [], dst: 2\n",
      "morphs: surface:ここ/base:ここ/pos:名詞/pos1:代名詞\n",
      "surface:で/base:で/pos:助詞/pos1:格助詞\n",
      "\n",
      "*  2\n",
      "srcs: [1], dst: 3\n",
      "morphs: surface:始め/base:始める/pos:動詞/pos1:自立\n",
      "surface:て/base:て/pos:助詞/pos1:接続助詞\n",
      "\n",
      "*  3\n",
      "srcs: [2], dst: 4\n",
      "morphs: surface:人間/base:人間/pos:名詞/pos1:一般\n",
      "surface:という/base:という/pos:助詞/pos1:格助詞\n",
      "\n",
      "*  4\n",
      "srcs: [3], dst: 5\n",
      "morphs: surface:もの/base:もの/pos:名詞/pos1:非自立\n",
      "surface:を/base:を/pos:助詞/pos1:格助詞\n",
      "\n",
      "*  5\n",
      "srcs: [0, 4], dst: -1\n",
      "morphs: surface:見/base:見る/pos:動詞/pos1:自立\n",
      "surface:た/base:た/pos:助動詞/pos1:*\n",
      "surface:。/base:。/pos:記号/pos1:句点\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    sentences = list(read_cabocha_file(neko_cabocha_file))\n",
    "    for i, chunk in enumerate(sentences[7]):\n",
    "        print('* ', i)\n",
    "        print('srcs: {}, dst: {}'.format(chunk.srcs, chunk.dst))\n",
    "        print('morphs: {}'.format(''.join([str(m) for m in chunk.morphs])))\n",
    "    import doctest\n",
    "    doctest.testmod()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 42. 係り元と係り先の文節の表示\n",
    "\n",
    "係り元の文節と係り先の文節のテキストをタブ区切り形式ですべて抽出せよ．ただし，句読点などの記号は出力しないようにせよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_src_and_dst_list = []\n",
    "paired_list = []\n",
    "paired_sentences = []\n",
    "\n",
    "# 一文ごとに処理\n",
    "for sentence in sentences:\n",
    "    if len(sentence) > 1:\n",
    "\n",
    "        for chunk in sentence:\n",
    "            # 係り元の文節と係り先の文節を抽出、\n",
    "            # dst: -1は係り先がないから飛ばす。\n",
    "            if chunk.dst == -1:\n",
    "                pass\n",
    "            else:\n",
    "                # 係り元\n",
    "                chunk_src = ''.join([m.surface for m in chunk.morphs])\n",
    "                # 係り先\n",
    "                chunk_dst = ''.join([m.surface for m in sentence[chunk.dst].morphs])\n",
    "\n",
    "                #句読点を出力しないようにする\n",
    "                chunk_src_and_dst = chunk_src + '\\t' + chunk_dst\n",
    "                chunk_src_and_dst = re.sub(r'[、。\\u3000]', '', chunk_src_and_dst)\n",
    "\n",
    "                chunk_src_and_dst_list.append(chunk_src_and_dst)\n",
    "                paired_list.append(chunk_src_and_dst)\n",
    "        \n",
    "        paired_sentences.append(chunk_src_and_dst_list) \n",
    "        chunk_src_and_dst_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['吾輩は\\t見た', 'ここで\\t始めて', '始めて\\t人間という', '人間という\\tものを', 'ものを\\t見た']\n",
      "吾輩は\t見た\n"
     ]
    }
   ],
   "source": [
    "print(paired_sentences[3])\n",
    "print(paired_sentences[3][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 「じめじめした所で」を文節に分けると、[じめじめした, 所で]であるが、出力結果は異なっている。これは係り受け解析器の精度の問題であろう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 43. 名詞を含む文節が動詞を含む文節に係るものを抽出\n",
    "\n",
    "名詞を含む文節が，動詞を含む文節に係るとき，これらをタブ区切り形式で抽出せよ．ただし，句読点などの記号は出力しないようにせよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_noun_to_verb_list = []\n",
    "\n",
    "# 一文ごとに処理\n",
    "for sentence in sentences:\n",
    "    \n",
    "    for chunk in sentence:\n",
    "        # 係り元の文節と係り先の文節を抽出、\n",
    "        # dst: -1は係り先がないから飛ばす。\n",
    "        if chunk.dst == -1:\n",
    "            pass\n",
    "        else:\n",
    "            \n",
    "            if ('名詞' in [m.pos for m in chunk.morphs]) & ('動詞' in [m.pos for m in sentence[chunk.dst].morphs]):\n",
    "                # 名詞\n",
    "                chunk_noun = ''.join([m.surface for m in chunk.morphs])\n",
    "                # 動詞\n",
    "                chunk_verb = ''.join([m.surface for m in sentence[chunk.dst].morphs])\n",
    "                \n",
    "                # タブ区切り、句読点を出力しないようにする\n",
    "                chunk_noun_to_verb = chunk_noun + '\\t' + chunk_verb\n",
    "                chunk_noun_to_verb = re.sub(r'[、。\\u3000]', '', chunk_noun_to_verb)\n",
    "                \n",
    "                chunk_noun_to_verb_list.append(chunk_noun_to_verb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['どこで\\t生れた', 'かとんと\\tつかぬ', '見当が\\tつかぬ', 'した所で\\t泣いて', 'いた事だけは\\t記憶している', '吾輩は\\t見た', 'ここで\\t始めて', 'ものを\\t見た', 'あとで\\t聞くと', '我々を\\t捕えて']\n",
      "28731\n"
     ]
    }
   ],
   "source": [
    "print(chunk_noun_to_verb_list[:10])\n",
    "print(len(chunk_noun_to_verb_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 44. 係り受け木の可視化\n",
    "\n",
    "与えられた文の係り受け木を有向グラフとして可視化せよ．可視化には，係り受け木を[DOT言語](http://ja.wikipedia.org/wiki/DOT言語)に変換し，[Graphviz](http://www.graphviz.org/)を用いるとよい．また，Pythonから有向グラフを直接的に可視化するには，[pydot](https://code.google.com/p/pydot/)を使うとよい．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydotplus as pdp\n",
    "\n",
    "def make_dot_edge(sentence):\n",
    "    body_edge = []\n",
    "    \n",
    "    for chunk_pair in sentence:\n",
    "        src = chunk_pair.split('\\t')[0]\n",
    "        dst = chunk_pair.split('\\t')[1]\n",
    "        \n",
    "        body_edge.append('{}->{}; '.format(src, dst))\n",
    "        \n",
    "    return body_edge\n",
    "\n",
    "def sentence_to_dot(idx, sentence):\n",
    "    head = \"digraph sentence{}\".format(idx)\n",
    "    #グラフを左から右に可視化\n",
    "    body_graph = \"{ graph [rankdir = LR];\"\n",
    "    body_edge = make_dot_edge(sentence)\n",
    "    \n",
    "    return head + body_graph + ''.join(body_edge) + '}'\n",
    "    \n",
    "\n",
    "def sentences_to_dots(sentences):\n",
    "    _dots = []\n",
    "    for idx, sentence in enumerate(sentences):\n",
    "        _dots.append(sentence_to_dot(idx, sentence))\n",
    "    \n",
    "    return _dots\n",
    "\n",
    "def save_graph(dot, filename):\n",
    "    g = pdp.graph_from_dot_data(dot)\n",
    "    g.write_jpeg(filename, prog='dot')\n",
    "    \n",
    "dots = sentences_to_dots(paired_sentences)\n",
    "for idx in range(101, 104):\n",
    "    save_graph(dots[idx], 'graph{}.jpg'.format(idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
